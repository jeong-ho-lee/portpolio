특징 공간 = 입력
2차원의  데이터를 벡터로써 표현한다

n차원의 데이터는 n차원 벡터로 표현한다
n차원 데이터가 직선 모델을 사용하는 경우 편차도 포함해야하기 때문에 매개변수의 수는 n + 1개가 된다

2차 곡선 모델일 때는 nC2 개의 항이 증가하기 때문에 n^2 + n + 1개의 매개변수가 필요하다

선형 분리 불가능한 공간을 선형 분리가 가능하게끔 변환된 특징 공간 (특징 공간 변환)을 만든다

1.2.3 왜 저따구로 변환을 하노? 특징 공간 변환의 식이 요점이 아니다 저 식은 특징 공간을 변환하여 선형 분리가 쉬워지게 할 수 있도록 하는 식이면 된다

변환 = 표현 학습이라고 한다

다수의 은닉층을 가진 계층적인 특징 공간 = 원래 변환을 한번만 하는데 딥러닝은 은닉층마다 변환을 조져서 더욱이 좋은 특징 공간을 찾는다
그니까 딥러닝은 점진적으로 좋은 특징 공간을 찾는 과정이라고 봐도 무방함

차원의 저주 = 계산량 증가 데이터 요구량이 터무니없이 증가한다
아무 특징이나 막 쓰지 말고 정말 중요한 특징들만 챙기자

실전적인 기계 학습 과정에서는 데이터의 생성 과정을 알 수 없기 때문에 양질의 데이터를 많이 모아야 추정 정확도가 높아진다

기계학습을 할 때 질이 양보다 중요하다
질이 안좋은 데이터가 하나라도 있으면 성능이 팍팍 떨어진다
데이터베이스 확보가 아주 중요하다

1.3.3 매니폴드 가정 = 고차원의 데이터를 데이터 공간에 뿌리면 샘플들을 잘 아우르는 Subspace가 있을거라고 가정하고 학습시키는 방법

회귀와 분류의 차이? 회귀는 예측값이 숫자 분류는 예측값이 숫자가 아니다 (ex 남자 여자, 개 고양이, 컴퓨터 입장에서는 아님) 크기가 정해져있는 숫자
훈련 집합 = 일반화하는 집합 테스트 집합 = 일반화 성능을 측정하는 집합
모델링 = 적절한 매개변수를 정하기 전에 모델의 형태를 정하는 과정 ex 뭐 직선 모델일지 2차 곡선 모델일지 편차는 넣을건지 등등
표현 학습 = 특징 변환을 통해서 좋은 특징 공간을 구하는 과정, 좋은 특징 공간 - 선형 분리가 가능한 특징 공간

특징 공간을 극 좌표계 변환을 통해서도 조작할 수 있다 ex) 한 데이터 군집이 다른 데이터 군집을 감싸고 있는 형태의 특징 공간

활성 함수 모델링 템플릿 같은 느낌인데 아직은 몰라도 될 듯 필요하면 교수님이 설명해주실 듯함