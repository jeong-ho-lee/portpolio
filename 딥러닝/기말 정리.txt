5.

기계 학습의 최적화가 어려운 이유

훈련 집합은 테스트 집합의 대리자
목적함수는 정확률의 대리자

목적함수의 비볼록, 고차원 특징 공간, 데이터 희소성

*MSE + Sigmoid < CE + Softmax
MSE보다 CE가 오차에 더 민감하다
Sigmoid : Gradient Vanishing

교차 엔트로피 - e = -(ylog2o + (1 - y)log2(1 - o)
로그우도 - e = -log2o(y)

데이터 전처리
규모 문제 - 단위에 의해 학습 속도에 차이가 발생할 수 있으므로 정규화 필요
모든 특징이 양수인 경우의 문제 - 가중치가 뭉치로 갱신되어 지그재그로 학습, 결론적으로 부호가 적절히 섞여야 됨
*정규화를 하면 평균을 빼고 다시 분산으로 나눠 평균을 0, 분산을 1로 만들어준다. 평균이 0이 되면 부호가 섞이게 되어 양수 문제를 해결하고 표준편차가 1이 되면 입력 특징 간 스케일 차이가 줄어든다
대칭적 가중치 문제 - 가중치가 동일한 노드는 업데이트도 동일하게 되어 노드의 개수가 여러개인 것이 의미가 없어지게 된다. 그래서 난수로 초기화하여 대칭을 파괴한다. [-r, r] 범위에서 랜덤하게 초기화하는데 *r은 입력 에지 개수에 반비례하도록 초기화해야 하고, 가중치가 대칭적이지 않도록 랜덤하게 초기화한다

모멘텀
v = αv - ρ ∂J / ∂Θ
Θ = Θ + v

v (속도 벡터)는 처음에 0으로 이전 그레디언트를 누적한 것에 해당한다.
α가 0이면 모멘텀이 없는 것과 마찬가지이고 1에 가까워질수록 매끄럽게 업데이트 된다 (지그재그 줄어듦) 보통 0.5, 0.9, 0.99 사용 (0.5로 시작해서 0.99에 도달). 오버슈팅 (*Gradient가 원치 않게 튀는 현상)을 누그러뜨림

네스테로프 (*속도 벡터로 ~Θ를 예견)
~Θ = Θ + αv
g = ∂J / ∂Θ | ~Θ
v = αv - ρg
Θ = Θ + v

적응적 학습률
학습률이 너무 크면 오버슈팅, 너무 작으면 수렴이 느림
AdaGrad
g = ∂J / ∂Θ | Θ
r = r + g ⨀ g
ΔΘ = -ρ / ϵ + √r ⨀ g (*r 값이 분모에 등장하여 g 값이 크다면 반영이 덜 되도록 했다)
ϵ는 divide by zero를 방지하는 아주 작은 수

r이 점점 커지면 수렴을 방해한다.
RMSProp
r = αr + (1 - α) g ⨀ g
α가 작을수록 최근 것에 비중을 둠 보통 0.9, 0.99, 0.999 사용

모멘텀 추가
Adam
v = 1 / 1 - (α1)^t v
r = 1 / 1 - (α2)^t r
초반에 잘 동작하게 하기 위해
α는 0.9, t는 Epoch 수

+ 네스테로프 생각
~Θ = Θ + α1v
g = ∂J / ∂Θ | ~Θ
v = α1v - ρg
v = 1 / 1 - (α1)^t v
r = α2r + (1 - α2) g ⨀ g
r = 1 / 1 - (α2)^t r
ΔΘ = -ρ / ϵ + √r v
Θ = Θ + ΔΘ

공변량 시프트 현상 - 학습 과정에서 샘플 분포가 바뀌는 현상, 배치 정규화 (위에 있는 데이터 셋 정규화랑 다른거임)
중간 결과 z, 미니 배치에 적용
*배치 정규화의 장점
규제효과
가중치 초기화에 덜 민감
학습률을 크게 하여 수렴 속도 향상 가능
Sigmoid 사용 가능

불량 문제 - 모델 용량에 비해 데이터가 부족한 경우
적절한 가정 투입 - 입력과 출력 사이의 매핑은 매끄럽다 (*매끄러움 가정 - 자연 상에 존재하는 대부분의 함수는 연속적이고 매끄럽다)

L2 규제 (리지 회귀) - 2ρλ 비율로 원점으로 끌어당김
^w = (XTX + 2λI)^-1 XTy
y = XT^w
L1 규제 (라쏘 회귀) - ρλ만큼 원점으로 끌어당김
희소성 효과 - 0이 되는 매개변수가 많다 = 모델의 구조적 용량을 줄일 수 있다
*L1 L2 차이 - L1이 더 Sparse함

조기 멈춤 - 몇 번 참다가 학습을 멈춤 (*q번마다 학습된 가중치로 검증, Validation Loss 증가하면 tol번 참다가 멈춤)

하이퍼 매개변수 최적화 - 임의 선택이 좋다

6.

k means 알고리즘
1. 군집 중심 초기화
2. 군집 배정
3. 군집 중심을 평균으로 대체
4. 2로 돌아가서 반복

오차함수는 dist로 함

k medoids는 평균 대신 대표 사용
k means에 비해 잡음에 둔감

욕심 알고리즘 - 각 단계에서 가장 좋아보이는 선택을 하는 방식으로 문제를 해결하는 알고리즘

밀도 추정
어떤 점 x에서 데이터가 발생할 확률

히스토그램 방법
특징 공간을 칸의 집합으로 분할한 다음, 칸에 있는 샘플의 빈도

커널 밀도 추정
커널 함수를 정하고 각 데이터별 커널 함수를 통과시킨 뒤 가중 합을 구하여 이를 데이터 수로 나눈다.
대역폭이 작으면 뾰족하고 크면 뭉개진다 - 적절한 설정 필요
처음부터 다 저장하고 있어야 됨, 데이터 희소성 - 낮은 차원인 경우만 활용

가우시안 혼합
커널 함수를 가우시안으로 정함. EM 알고리즘을 통해 학습

주성분 분석
손실을 최소화하면서 저차원으로 변환하는 것
*변환된 훈련집합의 분산이 클수록 정보손실이 적다고 판단 - 목표 = 분산을 최대화하는 축 찾기

*Langrangian Multiplier - 조건이 있는 최적화 문제 해결 f(x) + λ(g(x) - c)

공분산 행렬
평균 벡터 구하고
(xi - m)(xi - m)T
전부 더하고 개수로 나누면 공분산 행렬

공분산 행렬에 대해 고윳값 분해
det(Σ - λI) = 0
λ를 구하고 u를 구함
u에 내적해서 분산 구함

vs 독립 성분 분석
가우시안과 비상관 가정, 비가우시안과 독립성 가정
찾은 축 서로 수직, 찾은 축 서로 수직 x
차원 축소, 원음 분리

7.

표현의 가시화
필터 가시화
특징 맵 가시화 - 고양이 얼굴이 활성화된 특징 맵은 얼굴을 검출하는 특징 맵이라는 걸 시각적으로 확인
역투영 가시화
최적화 - 관찰 대상 노드 i에 영상 x가 입력되었을 때 활성을 ai(x)라고 하면 제일 잘 활성되는 x를 구함
이걸 경사 상승(오목)법으로 구함
디컨볼루션 - 관찰 대상 노드 i에서 컨볼루션의 역연산 적용하면 입력 이미지와 동일한 입력 패턴을 가진 이미지를 출력

*자가 학습
레이블이 있는 훈련 집합 Xl으로 학습기 학습
레이블이 없는 훈련 집합 Xu를 학습기로 분류
Xu에서 신뢰도 높은 일부 추출해서 Xl, Yl로 옮김
반복

협동 학습
자가 학습을 목적에 맞는 학습기 여러 개로 학습
각 학습기는 원래 각자의 부분 집합의 데이터를 분류하고 추출하여 전체 집합에 대해 갱신하지만
전체 집합을 학습기에 따라 나눠 각 부분 집합에서 추출한 데이터를 다른 부분 집합으로 교차로 옮기는 방식도 있다

*전이 학습 : 이미 학습된, 좋은 표현 공간을 가지고 있는 모델을 활용하여 학습
과업 전이 - 분야가 다름 ex. 영상 인식 vs 음성 인식
도메인 전이 - 분야는 같으나 특징 공간 ex. 한불 번역 vs 한영 번역 혹은 확률 분포(도메인 적응)가 다름 한 숫자 vs 미 숫자 MNIST

과업 전이 방식

동결 방식
훈련 집합을 특정 모델로 변환하여 변환된 훈련 집합으로 학습
전체 모델을 봤을 때 변환 모델의 가중치는 업데이트 되지 않으므로 동결층이라고 불림

미세 조정 방식
동결 방식이랑 똑같은데 변환 모델의 가중치도 업데이트됨
학습률이 높으면 가중치가 훼손되어 학습률을 낮게 설정해야 함

동결 vs 미세 조정
성능 <
속도 >
장단점 정도 생각

Daume
특징 공간을 둘 다, 원천, 목표 이렇게 3배로 확장
이후 화이트닝 (가우시안) 컬러링 (원천이 목표랑 같아지게끔)으로 확률 분포를 맞춤

9.

에이전트와 환경
에이전트는 정책으로 행동 결정
환경은 MDP (상태 s, 행동 a로 다음 상태 s', 보상 확률 r)로 상태, 보상 결정
목표는 누적 보상 최대화

탐사 vs 탐험
특정 vs 전체

*MDP의 통계적 특성 = 마르코프 성질 - 행동 결정에 이전 이력 상관 x

결정론적 MDP - 특정 상황에는 무조건 어떤 행동을 함
스토캐스틱 MDP - 돌발 행동

정책 - 어떤 상태에서 어떤 행동을 할 확률
최적 정책 ^π = argmax π goodness(π)

가치 함수 - 어떤 상태에서 종료까지 가능한 행동들에 대해서 확률 * 보상의 총합
^π = argmax π vπ(s), 모든 s (상태)에 대해
vπ(s) = Σp(z)r(z)

에피소드 과업 - 시작과 끝이 있음
영구 과업 - 무한 경로

영구 과업의 보상액은 무한등비급수 형태로 할인 누적 보상액

재귀 함수처럼 순환식 형태로 작성할 수 있음
이걸 이용해서 일반화하면
vπ(s) = Σp(a | s)(r + vπ(s')), 모든 s(상태)에 대해

스토캐스틱 + 영구 과업
vπ(s) = Σp(a|s) ΣΣp(s', r | s, a)(r + γvπ(s')), 모든 s(상태)에 대해