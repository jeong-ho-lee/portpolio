{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [SWCON253] Machine Learning\n",
    "Teaching Assistant: Yeongwoong Kim (duddnd7575@khu.ac.kr)\n",
    "\n",
    "Professor: Jinwoo Choi (jinwoochoi@khu.ac.kr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P7.A: PCA를 이용한 차원 축소 (5점)\n",
    "\n",
    "### 학습목표\n",
    "- PCA를 이용하는 방법을 익힌다.\n",
    "- PCA를 이용하여 차원축소를 수행할 수 있다.\n",
    "\n",
    "### 실습내용\n",
    "Scikit-Learn의 PCA를 이용하여 차원 축소를 수행합니다.\n",
    "\n",
    "실습은 다음 순서로 진행됩니다.\n",
    "- 1) 데이터셋 loading\n",
    "- 2) 모델 학습 **<직접 구현>**\n",
    "- 3) 모델 시각화\n",
    "- 4) Discussion\n",
    "\n",
    "**이번 실습에서 여러분은 `2)` 부분의 코드를 직접 작성합니다.**\n",
    "\n",
    "### 점수\n",
    "- 코드 구현: 2점, `#<your code>` 한 부분 마다 1점.\n",
    "- Discussion: 3점, 1번 1점 3번 2점.\n",
    "\n",
    "`.ipynb 파일과 함께 .html 파일 (File -> export as -> HTML)도 함께 제출하세요. 하나만 제출할시 감점이 있습니다.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Loading the Dataset and visualization\n",
    "랜덤으로 가우시안 분포의 데이터를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤으로 지정한 variance를 갖는 가우시안 분포의 데이터를 생성합니다.\n",
    "rng = np.random.RandomState(0)\n",
    "n_samples = 500\n",
    "cov = [[3, 3],\n",
    "       [3, 4]]\n",
    "X = rng.multivariate_normal(mean=[0, 0], cov=cov, size=n_samples)\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=.3, label='samples')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) PCA 학습\n",
    "\n",
    "PCA는 Principal component analysis의 약자로 한국말로 주성분 분석입니다.\n",
    "여기서 주성분(Principal component)란, 해당 데이터에 대해서 가장 큰 분산을 갖는 축의 방향 벡터를 의미합니다.\n",
    "\n",
    "예시와 함께 살펴보겠습니다.\n",
    "\n",
    "![](https://gblobscdn.gitbook.com/assets%2F-LvMRntv-nKvtl7WOpCz%2F-LvMRp9FltcwEeVxPYFs%2F-LvMRsnY_7eleRIHVcgg%2FPCAAnimation.gif?alt=media)\n",
    "\n",
    "위와 같은 분포의 데이터가 있을 때 두개의 주성분을 찾는다고 한다면, 데이터를 정사영(projection) 시켰을 때 ***가장 큰 분산***을 갖도록 하는 축의 방향 벡터가 바로 첫번째 주성분(분홍축)이 되게 됩니다.\n",
    "\n",
    "이어서 다음 주성분은 첫 번째 주성분에 ***수직***이면서 데이터를 정사형 시켰을때 가장 큰 분산을 갖도록하는 축이 두 번째 주성분이 되게 됩니다.\n",
    "\n",
    "---\n",
    "\n",
    "주성분 분석은 다양한 응용이 있습니다.\n",
    "- 고차원 데이터를 저차원으로 축소\n",
    "- 특징 벡터 추출\n",
    "- Eigen Face 추출을 통한 얼굴인식, 안경 제거 등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**생성한 2차원 평면의 데이터에 대한 주성분분석을 진행합니다. (Scikit-learn의 PCA 클래스 이용)**\n",
    "- n_components 를 데이터의 차원에 맞게 설정합니다.\n",
    "- fit 함수를 이용하여 학습을 진행합니다.\n",
    "- Scikit-learn documents를 참고하시기 바랍니다. https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PCA\n",
    "pca =   # <your code> PCA 클래스 생성\n",
    "        # <your code> 학습 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**주성분 분석을 통해 주성분, 즉 데이터의 분산이 가장 큰 방향 벡터와 주성분의 분산을 알 수 있습니다.**\n",
    "- PCA.components_는 주성분을 반환합니다.\n",
    "- PCA.explained_variance_은 주성분의 분산을 반환합니다.\n",
    "- 아래 셀을 실행시켜 이를 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Principal Components 1: {pca.components_[0]:}, Variance: {pca.explained_variance_[0]:.3f}')\n",
    "print(f'Principal Components 2: {pca.components_[1]:}, Variance: {pca.explained_variance_[1]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=.3, label='samples')\n",
    "\n",
    "for i, (comp, var) in enumerate(zip(pca.components_, pca.explained_variance_)):\n",
    "    # scale component by its variance explanation power\n",
    "    comp = comp * var  \n",
    "    plt.plot([0, comp[0]], [0, comp[1]], label=f\"Component {i}\", linewidth=5,\n",
    "             color=f\"C{i * 2 + 1}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Discussion\n",
    "\n",
    "**1) PCA에서 Eigen value는 무엇인지, Eigen value는 Principal Component와 어떤 차이점이 있는지 설명하세요. (1점)**\n",
    "\n",
    "**2) 4차원 데이터에 대해 PCA를 적용하여 얻은 Eigen value는 0.98, 0.84, 0.75, 0.2라고 가정했을 때, 몇개의 Principal Component를 선택하는게 효율적인 압축방법일까요? Principal Component의 갯수와 그만큼 선택한 이유를 설명하세요. (2점)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P7.B: PCA를 이용한 군집화 (5점)\n",
    "\n",
    "### 학습목표\n",
    "- PCA를 이용하여 IRIS데이터를 군집화 할 수 있다.\n",
    "- Scikit-Learn을 이용하여 모델 학습 과정을 구현할 수 있다.\n",
    "\n",
    "### 실습내용\n",
    "Scikit-Learn의 PCA를 이용하여 군집화가 가능한지 확인해 봅니다. \n",
    "\n",
    "실습은 다음 순서로 진행됩니다.\n",
    "- 1) 데이터셋 loading\n",
    "- 2) PCA 모델 구현, 학습, 시각화 **<직접 구현>**\n",
    "- 3) Discussion\n",
    "\n",
    "**이번 실습에서 여러분은 `2)` 부분의 코드를 직접 작성합니다.**\n",
    "\n",
    "\n",
    "### 점수\n",
    "- 모델 작성: 4점, `#<your code>` 한 부분 마다 1점.\n",
    "- Discussion: 1점\n",
    "\n",
    "`.ipynb 파일과 함께 .html 파일 (File -> export as -> HTML)도 함께 제출하세요. 하나만 제출할시 감점이 있습니다.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 데이터셋 loading\n",
    "앞의 실습에서 사용하였던 IRIS 데이터를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = # <your code> iris 데이터셋에서 데이터만 추출합니다. 이번 실습에서는 4개의 특징을 모두 사용합니다.\n",
    "y = # <your code> iris 데이터셋에서 타겟만 추출합니다.\n",
    "\n",
    "# 데이터셋 plot\n",
    "# 이 Plot은 scatter matrix 혹은 pairplot으로, 다변수 데이터셋에서 두개의 변수끼리 짝을지어 분포를 시각화하는 방법입니다.\n",
    "# 어떤 변수끼리가 상관성이 높은지 분포는 어떻게 되는지를 살펴볼 수 있습니다.\n",
    "colors=np.array(50*['r']+50*['g']+50*['b'])\n",
    "pd.plotting.scatter_matrix(pd.DataFrame(iris.data, columns=iris.feature_names), alpha=0.9, figsize=(10,10), color=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) PCA 모델 구현, 학습, 시각화\n",
    "\n",
    "PCA모델을 이용하여 위 데이터셋을 군집화해 봅니다.\n",
    "\n",
    "우리는 PCA가 고차원 정보를 저차원으로 축소한다는 사실을 배웠습니다. 하지만 단순히 저차원으로 축소하는 것이 아니라 분산이 최대가 되는 방향으로 축소를 하게 됩니다.  \n",
    "이러한 특징은 결과적으로 PCA가 일종의 클러스터링을 수행한다고도 볼 수 있습니다.\n",
    "\n",
    "![](https://i.stack.imgur.com/gZMOV.png)\n",
    "\n",
    "위 그림과 같이 3차원상의 데이터를 PCA를 통해 2차원으로 변환하면 아웃라이어를 제거하면서 모호한 경계를 더욱 뚜렷하게 변환하는 것을 볼 수 있습니다.  \n",
    "수학적인 수식이나 PCA와 클러스터링의 관계를 더 알아보려면 다음 링크를 참조하세요.\n",
    "\n",
    "**\\[더 읽어보기\\]** [PCA를 통해 클러스터링이 가능할까요? (PCA와 K-Means의 관계)](https://stats.stackexchange.com/questions/183236/what-is-the-relation-between-k-means-clustering-and-pca#:~:text=PCA%20is%20used%20to%20project%20the%20data%20onto%20two%20dimensions.&text=%22PCA%20aims%20at%20compressing%20the,the%20N%20data-points.%22)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4개의 특징갖는 IRIS 데이터를 PCA를 통해 3개의 특징으로 축소한 후, 각 데이터의 분포를 3D plot에서 살펴봅니다.**\n",
    "- n_components를 상황에 맞게 설정하여 PCA 모델을 생성합니다.\n",
    "- Documents를 참조하여 **학습 후 차원 축소된 데이터를 반환**하는 함수를 찾아 사용합니다.  \n",
    "- Scikit-learn documents를 참고하시기 바랍니다. https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA 학습\n",
    "pca =   # <your code> PCA 모델 생성\n",
    "X_hat = # <your code> 학습과 동시에 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "fig = plt.figure(1, figsize=(7, 7))\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134, auto_add_to_figure=False)    # define 3D axis\n",
    "fig.add_axes(ax)\n",
    "\n",
    "# Reorder the labels to have colors matching the cluster results\n",
    "y_hat = np.choose(y, [1, 2, 0]).astype(float)\n",
    "\n",
    "# show labels\n",
    "for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:\n",
    "    ax.text3D(X_hat[y_hat == label, 0].mean(),\n",
    "              X_hat[y_hat == label, 1].mean() + 1.5,\n",
    "              X_hat[y_hat == label, 2].mean(), name,\n",
    "              horizontalalignment='center',\n",
    "              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n",
    "\n",
    "\n",
    "ax.scatter(X_hat[:, 0], X_hat[:, 1], X_hat[:, 2], c=y_hat, cmap=plt.cm.plasma,\n",
    "           edgecolor='k')\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Discussion\n",
    "\n",
    "**1) PCA에는 다양한 응용 분야가 있습니다. 3개 이상 찾고 각 분야에서 어떻게 사용되는지 설명해 보세요. (1점)**\n",
    "- 1\n",
    "- 2\n",
    "- 3 \n",
    "\n",
    "**2) 구현하면서 든 질문에 대해 적고 답할 수 있다면 스스로 답해보세요.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
